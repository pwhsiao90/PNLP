{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 10 - Text Analytics & Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "* [電影劇本 Scripts analytics](#Movie-Script-Analytics)\n",
    "* [word2vec挖掘語義相似關係](#word2vec-movie-scripts)\n",
    "* [Load text file in folder to train Word2Vec model](#Load-text-file-in-folder-to-train-Word2Vec-model)\n",
    "* [回到電影劇本看人物Word2Vec的關聯性](#Back-to-Movie-Script-Word2Vec)\n",
    "* [Word2Vec調整參數](#Word2Vec-parameters)\n",
    "* [Train Beauty class Word2Vec model](#Beauty-Word2Vec-model)\n",
    "* [Recursively search Word2Vec words](#Recursively-search-Word2Vec-words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Movie-Script-Analytics\"></a>\n",
    "## 電影劇本 Scripts analytics\n",
    "\n",
    "流浪地球 The Wandering Earth\n",
    "改編自劉慈欣中短篇科幻小說作品。郭帆執導，吳京主演，描述中國人拯救世界的科幻電影，敘述太陽極速衰老膨脹，地球面臨被吞沒的可怕災難。為拯救地球，人類在地球表面上建造了上萬座行星發動機，以求逃離太陽系。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/movie_cover.jpg\" width=\"400\">\n",
    "    <center>Copyright & Disclaimer: All Pictures shown are for illustration purpose only</center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流浪地球劇本來源：https://www.bianju.me/Art_list.asp?ID=21729&page=18&CType=content\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "font_yahei_consolas = fm.FontProperties(fname='C:\\Windows\\Fonts\\kaiu.ttf',size=14)\n",
    "from matplotlib.font_manager import FontProperties\n",
    "myfont=FontProperties(fname=r'C:\\Windows\\Fonts\\kaiu.ttf',size=14)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def cut_document_in_list(text_list, stopwords, exception):\n",
    "    res_list = []\n",
    "    punct = set(u''':!),.:;?]}$¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "    punct |= set(exception)\n",
    "    for i in text_list:\n",
    "        # 以句號分句\n",
    "        w1 = i.split(\"。\")\n",
    "        for j in w1:\n",
    "            text = re.sub(r'，|。|？|：|“|”|！','',j.strip()) #刪除冊書符號\n",
    "            # w1 = re.sub(name1,name2,w1) #對齊\n",
    "            # w3 = list(jieba.cut(w2))#分詞\n",
    "            # w4 = [w for w in w3 if w not in stopword] #去掉停用詞\n",
    "            w4 = [word for word in jieba.cut(text, cut_all=False) if (len(word.strip()) >= 2) and (word not in stopwords) and ( not any(ext in word for ext in punct) )]\n",
    "            outstr = ''\n",
    "            for word in w4:\n",
    "                outstr +=word\n",
    "                outstr +=' '\n",
    "            if len(outstr.strip())>1:\n",
    "                res_list.append(outstr.strip())\n",
    "    return res_list\n",
    "\n",
    "stopwords = set()\n",
    "with open('dict/stopwords.txt', 'r', encoding=\"utf8\") as file:\n",
    "    stopwords = file.readlines()\n",
    "    stopwords = [stopword.strip('\\n').strip() for stopword in stopwords]\n",
    "except_file = open(\"dict/hippo_exception_word.txt\", encoding='utf-8')\n",
    "exception = except_file.read().split(',')\n",
    "exception.append(\" \")\n",
    "\n",
    "with open('dict/movie_cast_name.txt',encoding= 'utf-8') as f1:\n",
    "    data1 = f1.readlines()\n",
    "with open('data/scripts/流浪地球.txt',encoding= 'utf-8') as f2:\n",
    "    data2 = f2.readlines()\n",
    "data2 = [x.replace('\\n', '') for x in data2 if (len(x.strip()) >1)]\n",
    "with open('data/scripts/流浪地球.txt',encoding= 'utf-8') as f2:\n",
    "    data3 = f2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.initialize()\n",
    "jieba.load_userdict('dict/mydic2.txt')\n",
    "\n",
    "r = cut_document_in_list(data2, stopwords, exception)\n",
    "r[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 詞典裡的名字和劇本內容裡名字出現的次數\n",
    "count = []\n",
    "for name in data1:\n",
    "    #count.append([name.strip(), data3.count(name.strip())])\n",
    "    count.append([name.strip(), \" \".join(r).count(name.strip())])\n",
    "count1 = []\n",
    "for i in count:\n",
    "    if i not in count1:\n",
    "        count1.append(i)\n",
    "count = count1\n",
    "\n",
    "count.sort(key = lambda x:x[1])\n",
    "ay,ax = plt.subplots()\n",
    "numbers = [x[1] for x in count[-10:]]\n",
    "names = [x[0] for x in count[-10:]]\n",
    "ax.barh(range(10),numbers,color=['peru','coral'],align = 'center')\n",
    "ax.set_title('人物出場次數',fontsize = 14, fontproperties = font_yahei_consolas)\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels(names,fontsize = 14, fontproperties = font_yahei_consolas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人物出場次數 與社交人脈網路關係圖\n",
    "再來看看劇中人物的社交關係情況，採用以句為單位來進行分析（有時候也以段落為單位來識別人物關係，但採集的文本每集只有一個段落，所以不適用），即如果兩個人同時出現在一句話中，那說明他們之間肯定有某種關係。因此可以得到他們的社交網路關係。通過求得的共現矩陣，使用 Gephi 畫出下面的社交網路關係圖，圖中邊的粗細代表關係的密切程度，邊越粗表示兩人的關係越密切，而名字的大小可以表示為該人的社交人脈強弱情況。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#匹配詞典裡的名字和劇本內容裡的該名字出現的次數\n",
    "count = []\n",
    "for name in data1:\n",
    "    count.append([name.strip('\\n'), \" \".join(r).count(name.strip('\\n'))])\n",
    "count1 = []\n",
    "for i in count:\n",
    "    if i not in count1:\n",
    "        count1.append(i)\n",
    "count = count1\n",
    "\n",
    "count.sort(key = lambda x:x[1])\n",
    "ay,ax = plt.subplots()\n",
    "numbers = [x[1] for x in count[-10:]]\n",
    "names = [x[0].strip('\\n') for x in count[-10:]]\n",
    "ax.barh(range(10),numbers,color=['peru','coral'],align = 'center')\n",
    "ax.set_title('人物出場次數',fontsize = 14,fontproperties = font_yahei_consolas)\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels(names,fontsize = 14,fontproperties = font_yahei_consolas)\n",
    "\n",
    "#社交網路關係（共現矩陣）\n",
    "word = data2\n",
    "name = data1\n",
    "#name = data1[1:]\n",
    "#總人數\n",
    "wordcount = len(name) \n",
    "\n",
    "#初始化128*128值全為0的共現矩陣\n",
    "cormatrix = [[0 for col in range(wordcount)] for row in range(wordcount)] \n",
    "#遍歷矩陣行和列  \n",
    "for colindex in range(wordcount):\n",
    "    for rowindex in range(wordcount):\n",
    "        cornum = 0\n",
    "        #如果兩個人名字在同一句話裡出現，那麼共現矩陣中兩個人對應的值加1\n",
    "        for originline in word:\n",
    "            if name[colindex].strip('\\n') in originline and name[rowindex].strip('\\n') in originline:\n",
    "                cornum += 1\n",
    "        cormatrix[colindex][rowindex] = cornum\n",
    "\n",
    "cor_matrix = np.matrix(cormatrix)\n",
    "for i in range(len(name)):\n",
    "    cor_matrix[i,i] = 0\n",
    "social_cor_matrix = pd.DataFrame(cor_matrix, index = name, columns = name)\n",
    "#把共現矩陣存進excel\n",
    "social_cor_matrix.to_csv('report/movie/social_cor_matrix.csv')\n",
    "social_cor_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_contact = pd.DataFrame(columns = ['name1','name2','frequency'])\n",
    "#共現頻率\n",
    "for i in range(0,len(name)):\n",
    "    for j in range(0,len(name)):\n",
    "        if i<j and cormatrix[i][j] > 0:\n",
    "            social_contact.loc[len(social_contact),'name1'] = name[i].strip('\\n')\n",
    "            social_contact.loc[len(social_contact)-1,'name2'] = name[j].strip('\\n')\n",
    "            social_contact.loc[len(social_contact)-1,'frequency'] = cormatrix[i][j]\n",
    "\n",
    "social_contact.to_excel('report/movie/social_contact.xlsx',index = False)\n",
    "social_contact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![社交網路關係（共現矩陣）](images/movie_matrix_gephi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 被推走的 \"地球\" 共現文字雲\n",
    "\n",
    "接下來重點探索一下大衛比較感興趣的 \"地球\" 為本劇的主角 (被推走?!)，先通過關鍵字抓取出提及的劇本，然後使用 wordcloud 產生文字雲，wordcloud可以導入圖片自訂詞雲的形狀，非常方便，但是需要注意中文編碼和字體的問題，否則生成的圖片會顯示成亂碼。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/scripts/流浪地球.txt',encoding= 'utf-8') as f2:\n",
    "    data2 = f2.readlines()\n",
    "data2 = [x.replace('\\n', '') for x in data2 if (len(x.strip()) >1)]\n",
    "word = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#地球（感興趣的關鍵字，可能需要加入到詞典中）\n",
    "text = []\n",
    "#遍歷每句話\n",
    "for line in word:\n",
    "    if '地球' in line:\n",
    "        text.append(line)\n",
    "\n",
    "#詞頻統計\n",
    "dict_dz = {}\n",
    "for i in text:\n",
    "    dz1 = i.split(' ')\n",
    "    for w in dz1:\n",
    "        w1 = w.strip()\n",
    "        if dict_dz.__contains__(w1) :\n",
    "            dict_dz[w1] += 1\n",
    "        else:\n",
    "            dict_dz[w1] = 1     \n",
    "                            \n",
    "#生成text\n",
    "text1 = ''\n",
    "for i in text:\n",
    "    dz2 = i.split(' ')\n",
    "    for w in dz2:\n",
    "        text1 =text1 +' '+ w\n",
    "                \n",
    "#產生文字雲\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "\n",
    "# 設置字體格式路徑以顯示中文\n",
    "# 可以改成 'C:\\Windows\\Fonts\\xxxx.ttf' 來切換其他字體，在此大衛將ttf檔放在預設資料夾中\n",
    "# 選擇已經有的字體，根據詞頻否則生成圖片的時候會報錯：OSError: cannot open resource\n",
    "\n",
    "#讀取背景圖片資訊保存為array\n",
    "alice_mask = np.array(Image.open(\"images/cloud_mask7.png\"))\n",
    "font = 'font/simsun.ttf'\n",
    "wc = WordCloud(background_color = 'white', mask = alice_mask,\n",
    "               max_words = 2000, stopwords=STOPWORDS, font_path=font,\n",
    "               max_font_size = 80, random_state = 42,scale = 1.5).generate(text1)\n",
    "\n",
    "#store to file\n",
    "picture_name = \"images/movie_wordcloud.png\"\n",
    "wc.to_file(picture_name)\n",
    "\n",
    "# show word cloud\n",
    "#plt.rcParams[\"figure.figsize\"] = (100,80)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詞雲中詞語越大代表和大風廠這個詞相關度越高，可以看出和地球關係最緊密的有：流浪地球、劉啟、領航員、劉培強，或者遠一些的韓子昂、三百年後等\n",
    "回到前一session一開始提到的使用TF-IDF提取關鍵字，來看看電影劇本的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jieba import analyse\n",
    "\n",
    "tfidf = analyse.extract_tags\n",
    "#analyse.set_stop_words('stop_words.txt') #使用自定義的停用詞\n",
    "\n",
    "text_dz = ''\n",
    "for l in text:\n",
    "    text_dz += l\n",
    "    text_dz += ' '\n",
    "keywords = tfidf(text_dz,topK=20)\n",
    "print (keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "- choose your data set of movie script on internet\n",
    "- draw co-relation graph using Gephi software\n",
    "- draw a Word cloud for you to explore what is your finding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"word2vec-movie-scripts\"></a>\n",
    "## word2vec挖掘語義相似關係\n",
    "\n",
    "文章分析中也經常會用到word2vec將文字轉化為詞向量的形式，用以挖掘詞彙在語句前後的相似關係。使用word2vec測試一下模型效果。導入gensim 後，將文章轉化為模型所需的輸入形式，即可開始進行訓練，產出每個詞彙的向量格式，訓練模型的定義："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec（詞向量）](images/Word2Vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Word2Vec出來之前，我們常用的主要是one hot encoding的方法，也就是對於每一個單字進行編碼，即用在一個位置為1，其餘位置為0的向量來表示。\n",
    "向量的維度就是我們單字量的大小，向量的每一個位置，只能用來表示唯一的一個單字。\n",
    "\n",
    "舉個例子，假設我們的有詞庫只有10個單字，分別是:\n",
    "比，特，幣，昨，天，就，開，始，大，漲。 以one hot encoding的方法來表示每一個詞，那麼有下面的結果:\n",
    "\n",
    "\n",
    "比 → [1,0,0,0,0,0,0,0,0,0] <br>\n",
    "特 → [0,1,0,0,0,0,0,0,0,0] <br> \n",
    "幣 → [0,0,1,0,0,0,0,0,0,0] <br>\n",
    "昨 → [0,0,0,1,0,0,0,0,0,0] <br>\n",
    "天 → [0,0,0,0,1,0,0,0,0,0] <br>\n",
    "就 → [0,0,0,0,0,1,0,0,0,0] <br>\n",
    "開 → [0,0,0,0,0,0,1,0,0,0] <br>\n",
    "始 → [0,0,0,0,0,0,0,1,0,0] <br>\n",
    "大 → [0,0,0,0,0,0,0,0,1,0] <br>\n",
    "漲 → [0,0,0,0,0,0,0,0,0,1] <br>\n",
    "\n",
    "\n",
    "可以看到對於每一個單字，唯一的一個向量對它進行了表示。那麼很顯然這種表示方法至少有下面的一些缺陷\n",
    "\n",
    "1. 在常見的距離下面，單字與單字間的距離都是沒有差別的，\"比\"和\"特\"的距離和\"比\"和\"漲\"的距離是一樣的\n",
    "2. 隨著單字量的增加，向量的維度也隨之增加，對於詞庫中沒有的新字，無法以唯一的向量一對一的mapping對映\n",
    "3. 當單字量較多時，高維度向量對運算速度大大降低效率，無法適應於即時的訓練與作出調整\n",
    "\n",
    "所以，字向量的提出目的就是解決上面提到的這些問題。而英文可能是用於Word(字)，中文則是要以詞彙(至少兩個字)的向量來做訓練。\n",
    "\n",
    "1. 如果詞彙向量有N個，那麼可以用一個n維的向量來表示每一個詞彙，並且 n 遠遠小於N，常見的n為100到300維\n",
    "2. 詞彙向量每一個位置不再是只能取0和1的數值，而是可以取任意的實數\n",
    "3. 詞彙向量之間的差在一定程度上是有意義的，比如，台北的詞向量為 v1，台南的詞向量為 v2, 西雅圖的詞向量為 v3 , 紐約的詞向量為 v4 ，藉由word2vec學習出來的這些詞向量大致都有相符的特徵。這個近似關係，或是說城市關係，也正是受到word2vec的啟發，在知識圖譜表示學習中，衍生了一些名為Trans的編碼演算法\n",
    "4. 除此之外，地名和地名在詞向量空間中的距離比地名和動物的詞向量距離近，等等，換句話說就是描述同一屬性和種類的詞向量的距離要小於不同屬性和種類的詞向量的距離"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 轉成高維空間向量 - 以字為單位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = '結果鄭大仁看到芷芷留言後贊聲：「免神經，咖小而已。」未料此舉引起連千毅的小弟西門慶的留言，請鄭男刪除留言，但芷芷卻又突然冒出：「有人被威脅要泡茶拜訪。」、「我想請問他怎麼不用道歉啊？每次都要逼人要來我家拜訪，這樣很不好耶，我沒有惹到他啊！」, 此時連的小弟西門慶又回覆指：「這是多久的事情了何必提？無風不起浪，別惹事，我想應該是他身邊的人提醒妳，他本人沒空理妳。」也有網友留言：「整天被害妄想，腦袋是有事？」, 但芷芷似乎不想罷手，還繼續留言：「我為何要當替死鬼？我真的好難過呀，難道無法溝通嗎？」沒想到此話一出，引起雙方隔空互嗆，西門慶再嗆鄭又仁：「我跟你不認識，你為了不知名的整容女子來嗆聲，你混很好的樣子？」鄭又仁不滿對方抬他老大名號出來反嗆：「咖小就是咖小，一年賺100億也他家的事，惹到不該惹的人。」 自封「亞洲直播天王」的連千毅，這陣子引爆高雄街頭大戰，這場「直播主之亂」頻頻登上社會版面，名字也總跟酒色財氣、黑道連在一起，私下已婚的他，後宮也陷入駁火狀態，本刊接獲讀者爆料，已婚的連千毅加上正妹老婆，1人共連尬4女就算了，還撒出大筆鈔票，讓好幾個爆乳辣妹為他撕逼撕得厲害，其中一名「小四」情婦還是40歲的單親媽媽，爭風吃醋的字句流彈四射，本刊取得連千毅安撫這名「小四」的私密對話，看得出來連千毅不管在直播或女人，舌功都很了得。打開連千毅的直播，總是跟香車、名錶、金錢有關，擅於炒作話題的他，也懂美女吸睛的道理，每次直播，身邊總是坐著一到兩個爆乳辣妹，但這些爆乳妹不僅僅只是直播來賓，也猶如已婚的連千毅後宮佳麗一般，備受他金援以及寵愛。'\n",
    "model = Word2Vec(sentences, sg=1, size=100,  window=5,  min_count=1,  negative=3, sample=0.001, hs=1, workers=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢視字詞\n",
    "for i in range(9,20):\n",
    "    print(model.wv.index2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model['西']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示空間和字距離相近的字\n",
    "res = model.most_similar('西',topn = 100)  # 此字有出現在 corpus 當中\n",
    "for item in res[:5]:\n",
    "    print(\"{}, {}\".format(item[0], str(round(item[1], 2))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"word '西門' not in vocabulary\"\n",
    "vector = model['西門']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算兩字的相似度\n",
    "model.similarity('西', '小')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec 轉成高維空間向量 - 以詞彙為單位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "jieba.initialize()\n",
    "jieba.load_userdict('dict/mydic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jiebaCut(x):\n",
    "    punct = set(u''' :!),.:;?]}$¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "    # punct |= set(exception)\n",
    "    words = [word for word in jieba.cut(x, cut_all=False) if (len(word.strip()) >= 2) and ( not any(ext in word for ext in punct) )]\n",
    "    return words\n",
    "\n",
    "sentences_list = []\n",
    "sentences_list.append('結果鄭大仁看到芷芷留言後贊聲：「免神經，咖小而已。」未料此舉引起連千毅的小弟西門慶的留言，請鄭男刪除留言，但芷芷卻又突然冒出：「有人被威脅要泡茶拜訪。」、「我想請問他怎麼不用道歉啊？每次都要逼人要來我家拜訪，這樣很不好耶，我沒有惹到他啊！」, 此時連的小弟西門慶又回覆指：「這是多久的事情了何必提？無風不起浪，別惹事，我想應該是他身邊的人提醒妳，他本人沒空理妳。」也有網友留言：「整天被害妄想，腦袋是有事？」, 但芷芷似乎不想罷手，還繼續留言：「我為何要當替死鬼？我真的好難過呀，難道無法溝通嗎？」沒想到此話一出，引起雙方隔空互嗆，西門慶再嗆鄭又仁：「我跟你不認識，你為了不知名的整容女子來嗆聲，你混很好的樣子？」鄭又仁不滿對方抬他老大名號出來反嗆：「咖小就是咖小，一年賺100億也他家的事，惹到不該惹的人。」')\n",
    "sentences_list.append('自封「亞洲直播天王」的連千毅，這陣子引爆高雄街頭大戰，這場「直播主之亂」頻頻登上社會版面，名字也總跟酒色財氣、黑道連在一起，私下已婚的他，後宮也陷入駁火狀態，本刊接獲讀者爆料，已婚的連千毅加上正妹老婆，1人共連尬4女就算了，還撒出大筆鈔票，讓好幾個爆乳辣妹為他撕逼撕得厲害，其中一名「小四」情婦還是40歲的單親媽媽，爭風吃醋的字句流彈四射，本刊取得連千毅安撫這名「小四」的私密對話，看得出來連千毅不管在直播或女人，舌功都很了得。打開連千毅的直播，總是跟香車、名錶、金錢有關，擅於炒作話題的他，也懂美女吸睛的道理，每次直播，身邊總是坐著一到兩個爆乳辣妹，但這些爆乳妹不僅僅只是直播來賓，也猶如已婚的連千毅後宮佳麗一般，備受他金援以及寵愛。')\n",
    "\n",
    "sentences = list(map(jiebaCut, sentences_list))\n",
    "model = Word2Vec(sentences, size=100,  window=5,  min_count=1,  negative=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字以向量形式呈現\n",
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in model.wv.index2word[10:16]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示空間和字距離相近的詞彙\n",
    "res = model.most_similar('連千毅',topn = 100)  # 此詞彙有出現在 corpus 當中\n",
    "for item in res[:10]:\n",
    "    print(\"{}, {}\".format(item[0], str(round(item[1], 2))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 兩詞彙 Cosine 相似度\n",
    "res = model.similarity('連千毅', '正妹')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 三詞彙 Cosine 相似度\n",
    "res = model.most_similar(['連千毅', '西門慶'], ['芷芷'], topn= 100)\n",
    "print(\"%s之於%s，如%s之於: \" % ('連千毅', '西門慶', '芷芷'))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in res[:10]:\n",
    "    print(\"{}, {}\".format(item[0], str(item[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save news word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model/Word2Vec/word2vec_news.bin\"\n",
    "model.wv.save_word2vec_format(modelname, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load news word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, keyedvectors\n",
    "\n",
    "modelname = \"model/Word2Vec/word2vec_news.bin\"\n",
    "model = keyedvectors.KeyedVectors.load_word2vec_format(modelname,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 Word2Vec model\n",
    "res = model.similarity('連千毅', '正妹')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Load-text-file-in-folder-to-train-Word2Vec-model\"></a>\n",
    "## Load text file in folder to train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "jieba.initialize()\n",
    "jieba.load_userdict('dict/mydic.txt')\n",
    "\n",
    "class Sentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        jieba.load_userdict(\"dict/mydic.txt\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                punct = set(u''' :!),.:;?]}$¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "                # punct |= set(exception)\n",
    "                words = [word for word in jieba.cut(line, cut_all=False) if (len(word.strip()) >= 2) and ( not any(ext in word for ext in punct) )]\n",
    "                yield words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 歌詞\n",
    "folder_path = \"./data/lyrics/\"\n",
    "sentences = Sentences(folder_path)\n",
    "model = Word2Vec(sentences, size=100,  window=5,  min_count=0,  negative=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index2word[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save lyrics Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model/Word2Vec/word2vec_lyrics.bin\"\n",
    "model.wv.save_word2vec_format(modelname, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load lyrics Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, keyedvectors\n",
    "\n",
    "modelname = \"model/Word2Vec/word2vec_lyrics.bin\"\n",
    "model = keyedvectors.KeyedVectors.load_word2vec_format(modelname,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試訓練好的詞向量模型，使用 model[keyWord] 取出 keyword 這個詞彙的詞向量\n",
    "keyWord = '眼淚'\n",
    "print(model[keyWord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何評價所建「詞向量」的好壞？\n",
    "\n",
    "依照 word2vec 的原理，詞意相近的詞在向量空間當中的距離是接近的，但會因為「文本內容性質的差異」而有所不同，例如在新聞類的文本中，「台灣」字詞會常與地名或時事事件等字詞距離接近；然而若是跟 PTT 論壇相關的文本，「台灣」字詞可能會更常與「鄉民/島民/魯蛇/溫拿」這類聊天用語的字詞距離靠近；總之，可以透過所訓練之詞向量距離，觀察他們語意是否相近去做衡量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示空間和字距離相近的關聯詞彙\n",
    "res = model.most_similar('懷念',topn = 100)  # 此詞彙有出現在 corpus 當中\n",
    "for item in res[:10]:\n",
    "    print(\"{}, {}\".format(item[0], str(round(item[1], 2))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('懷念', '那年')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Back-to-Movie-Script-Word2Vec\"></a>\n",
    "## 回到 電影流浪地球劇本 看人物 Word2Vec 的關聯性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "jieba.initialize()\n",
    "jieba.load_userdict('dict/movie_cast_name.txt')\n",
    "\n",
    "stopwords = set()\n",
    "with open('dict/stopwords.pkl', 'rb') as handle:\n",
    "    stopwords = pickle.load(handle)\n",
    "\n",
    "class Sentences(object):\n",
    "    def __init__(self, dirname, stopwords):\n",
    "        self.dirname = dirname\n",
    "        jieba.load_userdict(\"dict/mydic.txt\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                punct = set(u''' :!),.:;?]}$¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "                # punct |= set(exception)\n",
    "                words = [word for word in jieba.cut(line, cut_all=False) if (len(word.strip()) >= 2) and ( not any(ext in word for ext in punct) )]\n",
    "                yield words\n",
    "\n",
    "def cal_word2vec(word2vec_model, word, topn):\n",
    "    try:\n",
    "        vector = word2vec_model[word]\n",
    "        res = model.most_similar(word, topn=topn)  # 此詞彙有出現在 corpus 當中\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return \"word '{}' not in vocabulary\".format(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 電影\n",
    "folder_path = \"./data/scripts/\"\n",
    "sentences = Sentences(folder_path, stopwords)\n",
    "model = Word2Vec(sentences, size=100,  window=5,  min_count=5,  negative=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save movie Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model/Word2Vec/word2vec_movie.bin\"\n",
    "model.wv.save_word2vec_format(modelname, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, keyedvectors\n",
    "\n",
    "modelname = \"model/Word2Vec/word2vec_movie.bin\"\n",
    "model = keyedvectors.KeyedVectors.load_word2vec_format(modelname,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (model['地球'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"地球\"\n",
    "keyword = \"劉啟\"\n",
    "res = cal_word2vec(model, keyword, 100)\n",
    "for item in res[:10]:\n",
    "    print(\"{}, {}\".format(item[0], str(round(item[1], 2))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Word2Vec-parameters\"></a>\n",
    "## Word2Vec 調整參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from gensim.models import Word2Vec  \n",
    "model = Word2Vec(sentences, sg=1, size=100,  window=5,  min_count=5,  negative=3, sample=0.001, hs=1, workers=4)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數解釋：\n",
    "# 其中的sentences是句子組成的 list，而每個句子又是詞彙的列表(非一句話或一篇文章)，即 list[list] 類型。\n",
    "\n",
    "#1.sg=1是skip-gram演算法，對低頻詞敏感；預設sg=0為CBOW演算法。\n",
    "\n",
    "#2.size是輸出詞向量的維數，值太小會導致詞映射因為衝突而影響結果，值太大則會耗記憶體並使演算法計算變慢，一般值取為100到200之間。\n",
    "\n",
    "#3.window是句子中當前詞與目標詞之間的最大距離，3表示在目標詞前看3-b個詞，後面看b個詞（b在0-3之間隨機）。\n",
    "\n",
    "#4.min_count是對詞進行過濾，詞頻小於 min-count 的單詞則會被忽略，預設值為5。\n",
    "\n",
    "#5.negative和sample可根據訓練結果進行微調，sample表示更高頻率的詞被隨機下採樣到所設置的閾值，預設值為1e-3。\n",
    "\n",
    "#6.hs=1展示層級softmax將會被使用，預設 hs=0 且 negative 不為 0，則負採樣將會被選擇使用。\n",
    "\n",
    "#7.workers控制訓練的平行處理，此參數只有在Windows安裝了Cython或Linux環境才有效，否則只能使用單核，anaconda 會內建 Cython。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Beauty-Word2Vec-model\"></a>\n",
    "##  Train Beauty class Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\princ\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.813 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "jieba.initialize()\n",
    "jieba.set_dictionary(\"dict/dict.big.txt\")\n",
    "\n",
    "stopwords = set()\n",
    "with open('dict/stopwords.pkl', 'rb') as handle:\n",
    "    stopwords = pickle.load(handle)\n",
    "\n",
    "class Sentences(object):\n",
    "    def __init__(self, dirname, stopwords):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='utf-8'):\n",
    "                punct = set(u''' :!),.:;?]}$¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "                # punct |= set(exception)\n",
    "                words = [word for word in jieba.cut(line, cut_all=False) if (len(word.strip()) >= 2) and (word not in stopwords) and ( not any(ext in word for ext in punct) )]\n",
    "                yield words\n",
    "\n",
    "def cal_word2vec(word2vec_model, word, topn):\n",
    "    try:\n",
    "        vector = word2vec_model[word]\n",
    "        res = model.most_similar(word, topn=topn)  # 此詞彙有出現在 corpus 當中\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return \"word '{}' not in vocabulary\".format(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Beauty class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\Programming\\Python\\課程教學\\David Python 教育訓練\\dict\\dict.big.txt ...\n",
      "Loading model from cache C:\\Users\\princ\\AppData\\Local\\Temp\\jieba.u9fe11b2227c896c5bbad65bb17730314.cache\n",
      "Loading model cost 1.496 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 美妝\n",
    "folder_path = \"./data/corpus/beauty_small/\"\n",
    "sentences = Sentences(folder_path, stopwords)\n",
    "model = Word2Vec(sentences, size=240,  window=5,  min_count=5,  negative=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Beauty class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"model/Word2Vec/beauty_small.bin\"\n",
    "model.wv.save_word2vec_format(modelname, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Beauty class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, keyedvectors\n",
    "\n",
    "modelname = \"model/Word2Vec/beauty_small.bin\"\n",
    "model = keyedvectors.KeyedVectors.load_word2vec_format(modelname,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['彩妝', '唇膏', '肌膚', '活動', '保濕', '保養', '面膜', '眼影', '限量', '使用']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Recursively-search-Word2Vec-words\"></a>\n",
    "## Recursively search Word2Vec words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_relation_keywords(model, query):\n",
    "    try:\n",
    "        list_new  = []\n",
    "        layer_1 = list([])\n",
    "        layer_2 = list([])\n",
    "        res = model.most_similar(query, topn = 20)\n",
    "        for item in res:\n",
    "            similarity = item[1]\n",
    "            if similarity > 0.5:\n",
    "                child = item[0]\n",
    "                layer_1.append(child)\n",
    "                layer_2.append(child)\n",
    "\n",
    "        for child in layer_1:\n",
    "            res = model.most_similar(child, topn = 23)\n",
    "            for item in res:\n",
    "                similarity = item[1]\n",
    "                if similarity > 0.5:\n",
    "                    child = item[0]\n",
    "                    if child not in layer_2:\n",
    "                        layer_2.append(child)\n",
    "\n",
    "            # Homework Hint:\n",
    "            #for child in layer_2:\n",
    "            #    res = model.most_similar(child, topn = 23)\n",
    "            #    for item in res:\n",
    "            #        similarity = item[1]\n",
    "            #        if similarity > 0.5:\n",
    "            #            child = item[0]\n",
    "\n",
    "        print(len(layer_1))\n",
    "        print(layer_1)\n",
    "        print('='*20)\n",
    "        print(len(layer_2))\n",
    "        #print(layer_2)\n",
    "        result_list = []\n",
    "        for x in layer_2:\n",
    "            result_list.append(x)\n",
    "        print(result_list)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception:'+repr(e))\n",
    "    else:\n",
    "        pass\n",
    "    finally:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "['肌膚', '唇膏', '使用', '彩妝', '底妝', '粉底', '眼影', '產品', '皮膚', '保養', '保濕', '面膜', '唇彩', '效果', '精華', '優惠', '限定', '商品', '秋冬', '系列']\n",
      "====================\n",
      "40\n",
      "['肌膚', '唇膏', '使用', '彩妝', '底妝', '粉底', '眼影', '產品', '皮膚', '保養', '保濕', '面膜', '唇彩', '效果', '精華', '優惠', '限定', '商品', '秋冬', '系列', '這次', '蜜粉', '修護', '品牌', '限量', '推出', '小編', '完美', '日本', '美妝', '聖誕', '包裝', '自然', '推薦', '粉底液', '護膚', '精華液', '適合', '活動', '週年']\n"
     ]
    }
   ],
   "source": [
    "query = '化妝'\n",
    "gen_relation_keywords(model, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "- 請實作加上第三層\n",
    "- 加上第三層的關聯效果如何?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
