{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 14 - RNN Poet generator & Chatbot\n",
    "\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "* [Train poet generator model using Tensorflow](#rnn-poems)\n",
    "* [Generate Poems Chatbot](#poems-chatbot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rnn-poems\"></a>\n",
    "## Train poet generator model using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐詩數量: 34646\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.RNNPoet.poetry_porcess import *\n",
    "from src.RNNPoet.gen_poetry import *\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "batch_size = 64\n",
    "poetry_file = './data/poems/poems.txt'\n",
    "\n",
    "def train(words,poetry_vector,x_batches,y_batches):\n",
    "    input_data = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    output_targets = tf.placeholder(tf.int32,[batch_size,None])\n",
    "    end_points = rnn_model(len(words),input_data=input_data,output_data=output_targets,batch_size=batch_size)\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    merge = tf.summary.merge_all()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        writer = tf.summary.FileWriter('./logs',sess.graph)\n",
    "        sess.run(init_op)\n",
    "\n",
    "        start_epoch = 0\n",
    "        model_dir = \"./model/RNNPoems\"\n",
    "        epochs = 50\n",
    "        checkpoint = tf.train.latest_checkpoint(model_dir)\n",
    "        if checkpoint:\n",
    "            saver.restore(sess,checkpoint)\n",
    "            print(\"## restore from the checkpoint {0}\".format(checkpoint))\n",
    "            start_epoch += int(checkpoint.split('-')[-1])\n",
    "            print('## start training...')\n",
    "        try:\n",
    "            for epoch in range(start_epoch,epochs):\n",
    "                n_chunk = len(poetry_vector) // batch_size\n",
    "                for n in range(n_chunk):\n",
    "                    loss,_,_ = sess.run([\n",
    "                        end_points['total_loss'],\n",
    "                        end_points['last_state'],\n",
    "                        end_points['train_op'],\n",
    "                    ],feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                    print('Epoch: %d, batch: %d, training loss: %.6f' % (epoch,n,loss))\n",
    "                    if epoch % 5 == 0:\n",
    "                        saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "                        result = sess.run(merge,feed_dict={input_data: x_batches[n],output_targets: y_batches[n]})\n",
    "                        writer.add_summary(result,epoch * n_chunk + n)\n",
    "        except KeyboardInterrupt:\n",
    "            print('## Interrupt manually, try saving checkpoint for now...')\n",
    "            saver.save(sess,os.path.join(model_dir,\"poetry\"),global_step=epoch)\n",
    "            print('## Last epoch were saved, next time will start from epoch {}.'.format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    words,poetry_vector,to_num,x_batches,y_batches = poetry_process()\n",
    "    #train(words, poetry_vector, x_batches, y_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNNPoet_TensorFlow_model.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "def rnn_model(num_of_word,input_data,output_data=None,rnn_size=128,num_layers=2,batch_size=128):\n",
    "    end_points = {}\n",
    "    \"\"\"\n",
    "\n",
    "    :param num_of_word: 詞彙的個數\n",
    "    :param input_data: 輸入向量\n",
    "    :param output_data: 標籤\n",
    "    :param rnn_size: 隱藏層的向量大小\n",
    "    :param num_layers: 隱藏層的階層數\n",
    "    :param batch_size: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Construct RNN '''\n",
    "    # cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    # cell_fun = tf.contrib.rnn.GRUCell\n",
    "    cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "    cell = cell_fun(rnn_size,state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers,state_is_tuple=True)\n",
    "\n",
    "    # 如果發現標籤(output_data)，則初始化為一個 batch cell\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1,tf.float32)\n",
    "\n",
    "    # 崁入詞向量\n",
    "    embedding = tf.get_variable('embedding',initializer=tf.random_uniform(\n",
    "        [num_of_word + 1,rnn_size],-1.0,1.0))\n",
    "    inputs = tf.nn.embedding_lookup(embedding,input_data)\n",
    "\n",
    "    outputs,last_state = tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state)\n",
    "    output = tf.reshape(outputs,[-1,rnn_size])\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size,num_of_word + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[num_of_word + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output,weights),bias=bias)\n",
    "\n",
    "    if output_data is not None:\n",
    "        labels = tf.one_hot(tf.reshape(output_data,[-1]),depth=num_of_word + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n",
    "        tf.summary.scalar('loss',total_loss)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else:\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gen_poetry.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    " \n",
    "from src.RNNPoet.LSTM_model import rnn_model\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "def to_word(predict, vocabs):\n",
    "    t = np.cumsum(predict)\n",
    "    s = np.sum(predict)\n",
    "    sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]  # [np.argmax(predict)]\n",
    "\n",
    "def gen_poetry(words, to_num):\n",
    "    batch_size = 1\n",
    "    print('儲存模型為: {}'.format('./model'))\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    " \n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        word = input('請輸入起始句: ')\n",
    "        poem_ = ''\n",
    "        while word != 'E':\n",
    "            poem_ += word\n",
    "            x = np.array(to_num(word)).reshape(1, 1)\n",
    "            predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                           feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            word = to_word(predict, words)\n",
    "        print(poem_)\n",
    "        return poem_\n",
    "\n",
    "def generate_poet(start_with, words, to_num, style_words=\"誰謂傷心畫不成，畫人心逐世人情。\"):\n",
    " \n",
    "    batch_size = 1\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        if style_words:\n",
    "            for word in style_words:\n",
    "                x = np.array(to_num(word)).reshape(1, 1)\n",
    "                last_state = sess.run(end_points['last_state'],\n",
    "                                      feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    " \n",
    "        start_words = list(start_with)\n",
    "        start_word_len = len(start_words)\n",
    " \n",
    "        result = start_words.copy()\n",
    "        max_len = 200\n",
    "        for i in range(max_len):\n",
    " \n",
    "            if i < start_word_len:\n",
    "                w = start_words[i]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            else:\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "                w = to_word(predict, words)\n",
    "                # w = words[np.argmax(predict)]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                if w == 'E':\n",
    "                    break\n",
    "                result.append(w)\n",
    "        print(''.join(result))\n",
    "        return ''.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poetry_porcess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def poetry_process(batch_size=64,\n",
    "                   poetry_file='./data/poems/poems.txt'):\n",
    "    poetrys = []\n",
    "    with open(poetry_file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                title,content = line.strip().split(':')\n",
    "                content = content.replace(' ','')  # 去除空白字元\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 79:\n",
    "                    continue\n",
    "                content = 'B' + content + 'E'\n",
    "                poetrys.append(content)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # 依據每首詩的長度排序\n",
    "    # poetrys = sorted(poetrys, key=lambda poetry: len(poetry))\n",
    "    print('唐詩數量:',len(poetrys))\n",
    "\n",
    "    # 统计字出现次数\n",
    "    all_words = []\n",
    "    for poetry in poetrys:\n",
    "        all_words += [word for word in poetry]\n",
    "    counter = Counter(all_words)\n",
    "    # print(counter.items())\n",
    "    # item 會將字典中的每一項，轉置為一個二元 byte，字典變成大的list\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: -x[1])\n",
    "    # 使用 zip 取出，由於原始資料的結構，不如numpy的結構好用\n",
    "    words,_ = zip(*count_pairs)\n",
    "    # print(words)\n",
    "\n",
    "    words = words[:len(words)] + (' ',)  # 在每個list後便，新增一個空白字元' '來補齊詩句的長度\n",
    "    # print(words)\n",
    "    # 字典: word->int\n",
    "    word_num_map = dict(zip(words,range(len(words))))\n",
    "    # 將詩詞轉換為向量格式\n",
    "    to_num = lambda word: word_num_map.get(word,len(words))\n",
    "    poetry_vector = [list(map(to_num,poetry)) for poetry in poetrys]\n",
    "\n",
    "    n_chunk = len(poetry_vector) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batches = poetry_vector[start_index:end_index]\n",
    "        length = max(map(len,batches))  # 记录下最长的诗句的长度\n",
    "        xdata = np.full((batch_size,length),word_num_map[' '],np.int32)\n",
    "        for row in range(batch_size):\n",
    "            xdata[row,:len(batches[row])] = batches[row]\n",
    "        # print(len(xdata[0])) 每个batch中数据长度不相等\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:,:-1] = xdata[:,1:]\n",
    "        \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "            \"\"\"\n",
    "        x_batches.append(xdata)  # (n_chunk, batch, length)\n",
    "        y_batches.append(ydata)\n",
    "    return words,poetry_vector,to_num,x_batches,y_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "batch_size = 64\n",
    "poetry_file = './data/poems/poems.txt'\n",
    "\n",
    "def to_word(predict, vocabs):\n",
    "    t = np.cumsum(predict)\n",
    "    s = np.sum(predict)\n",
    "    sample = int(np.searchsorted(t, np.random.rand(1) * s))\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]  # [np.argmax(predict)]\n",
    "\n",
    "def gen_poetry(words, to_num):\n",
    "    batch_size = 1\n",
    "    print('儲存模型為: {}'.format('./model'))\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    " \n",
    "        checkpoint = tf.train.latest_checkpoint('./model')\n",
    "        saver.restore(sess, checkpoint)\n",
    " \n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    " \n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    " \n",
    "        word = input('請輸入起始句: ')\n",
    "        poem_ = ''\n",
    "        while word != 'E':\n",
    "            poem_ += word\n",
    "            x = np.array(to_num(word)).reshape(1, 1)\n",
    "            predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                           feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            word = to_word(predict, words)\n",
    "        print(poem_)\n",
    "        return poem_\n",
    "\n",
    "def poetry_process(batch_size=64,\n",
    "                   poetry_file='./data/poems/poems.txt'):\n",
    "    poetrys = []\n",
    "    with open(poetry_file,'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                title,content = line.strip().split(':')\n",
    "                content = content.replace(' ','')  # 去除空白字元\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 79:\n",
    "                    continue\n",
    "                content = 'B' + content + 'E'\n",
    "                poetrys.append(content)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    # 依據每首詩的長度排序\n",
    "    # poetrys = sorted(poetrys, key=lambda poetry: len(poetry))\n",
    "    print('唐詩數量:',len(poetrys))\n",
    "\n",
    "    # 统计字出现次数\n",
    "    all_words = []\n",
    "    for poetry in poetrys:\n",
    "        all_words += [word for word in poetry]\n",
    "    counter = Counter(all_words)\n",
    "    # print(counter.items())\n",
    "    # item 會將字典中的每一項，轉置為一個二元 byte，字典變成大的list\n",
    "    count_pairs = sorted(counter.items(),key=lambda x: -x[1])\n",
    "    # 使用 zip 取出，由於原始資料的結構，不如numpy的結構好用\n",
    "    words,_ = zip(*count_pairs)\n",
    "    # print(words)\n",
    "\n",
    "    words = words[:len(words)] + (' ',)  # 在每個list後便，新增一個空白字元' '來補齊詩句的長度\n",
    "    # print(words)\n",
    "    # 字典: word->int\n",
    "    word_num_map = dict(zip(words,range(len(words))))\n",
    "    # 將詩詞轉換為向量格式\n",
    "    to_num = lambda word: word_num_map.get(word,len(words))\n",
    "    poetry_vector = [list(map(to_num,poetry)) for poetry in poetrys]\n",
    "\n",
    "    n_chunk = len(poetry_vector) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        batches = poetry_vector[start_index:end_index]\n",
    "        length = max(map(len,batches))  # 记录下最长的诗句的长度\n",
    "        xdata = np.full((batch_size,length),word_num_map[' '],np.int32)\n",
    "        for row in range(batch_size):\n",
    "            xdata[row,:len(batches[row])] = batches[row]\n",
    "        # print(len(xdata[0])) 每个batch中数据长度不相等\n",
    "        ydata = np.copy(xdata)\n",
    "        ydata[:,:-1] = xdata[:,1:]\n",
    "        \"\"\"\n",
    "            xdata             ydata\n",
    "            [6,2,4,6,9]       [2,4,6,9,9]\n",
    "            [1,4,2,8,5]       [4,2,8,5,5]\n",
    "            \"\"\"\n",
    "        x_batches.append(xdata)  # (n_chunk, batch, length)\n",
    "        y_batches.append(ydata)\n",
    "    return words,poetry_vector,to_num,x_batches,y_batches\n",
    "\n",
    "def rnn_model(num_of_word,input_data,output_data=None,rnn_size=128,num_layers=2,batch_size=128):\n",
    "    end_points = {}\n",
    "    \"\"\"\n",
    "\n",
    "    :param num_of_word: 詞彙的個數\n",
    "    :param input_data: 輸入向量\n",
    "    :param output_data: 標籤\n",
    "    :param rnn_size: 隱藏層的向量大小\n",
    "    :param num_layers: 隱藏層的階層數\n",
    "    :param batch_size: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    ''' Construct RNN '''\n",
    "    # cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    # cell_fun = tf.contrib.rnn.GRUCell\n",
    "    cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "\n",
    "    cell = cell_fun(rnn_size,state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers,state_is_tuple=True)\n",
    "\n",
    "    # 如果發現標籤(output_data)，則初始化為一個 batch cell\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1,tf.float32)\n",
    "\n",
    "    # 崁入詞向量\n",
    "    embedding = tf.get_variable('embedding',initializer=tf.random_uniform(\n",
    "        [num_of_word + 1,rnn_size],-1.0,1.0))\n",
    "    inputs = tf.nn.embedding_lookup(embedding,input_data)\n",
    "\n",
    "    outputs,last_state = tf.nn.dynamic_rnn(cell,inputs,initial_state=initial_state)\n",
    "    output = tf.reshape(outputs,[-1,rnn_size])\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size,num_of_word + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[num_of_word + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output,weights),bias=bias)\n",
    "\n",
    "    if output_data is not None:\n",
    "        labels = tf.one_hot(tf.reshape(output_data,[-1]),depth=num_of_word + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(0.01).minimize(total_loss)\n",
    "        tf.summary.scalar('loss',total_loss)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else:\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐詩數量: 34646\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-6-d20157a52aaf>:131: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-d20157a52aaf>:132: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-d20157a52aaf>:145: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "# 這個 cell 只要執行一次即可\n",
    "words,poetry_vector,to_num,x_batches,y_batches = poetry_process()\n",
    "\n",
    "batch_size = 1\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "end_points = rnn_model(len(words), input_data=input_data, batch_size=batch_size)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poet(start_with, style_words=\"誰謂傷心畫不成，畫人心逐世人情。\"):\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(init_op)\n",
    "        checkpoint = tf.train.latest_checkpoint('./model/RNNPoems')\n",
    "        saver.restore(sess, checkpoint)\n",
    "        x = np.array(to_num('B')).reshape(1, 1)\n",
    "        _, last_state = sess.run([end_points['prediction'], end_points['last_state']], feed_dict={input_data: x})\n",
    "        if style_words:\n",
    "            for word in style_words:\n",
    "                x = np.array(to_num(word)).reshape(1, 1)\n",
    "                last_state = sess.run(end_points['last_state'],\n",
    "                                      feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "        start_words = list(start_with)\n",
    "        start_word_len = len(start_words)\n",
    "        result = start_words.copy()\n",
    "        max_len = 200\n",
    "        for i in range(max_len):\n",
    "            if i < start_word_len:\n",
    "                w = start_words[i]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            else:\n",
    "                predict, last_state = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                               feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "                w = to_word(predict, words)\n",
    "                # w = words[np.argmax(predict)]\n",
    "                x = np.array(to_num(w)).reshape(1, 1)\n",
    "                if w == 'E':\n",
    "                    break\n",
    "                result.append(w)\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動產生詩詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model/RNNPoems\\poetry-45\n",
      "少小離家老大回淚，青春多雨白團團。\n"
     ]
    }
   ],
   "source": [
    "start_with = '少小離家老大回'\n",
    "r = generate_poet(start_with, style_words=\"大漠孤煙直，長河落日圓。\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/RNNPoems\\poetry-45\n",
      "天龍國大戰南北，貔虎功夷瑞遍雕。膏圓駘蕩火仙手，紅杏輕含秋色青。應擁薊門秋雨曲，妒吹金蹙鑒梨枝。\n"
     ]
    }
   ],
   "source": [
    "start_with = '天龍國大戰南北'\n",
    "r = generate_poet(start_with, style_words=\"大漠孤煙直，長河落日圓。\")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"poems-chatbot\"></a>\n",
    "## Generate Poems Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "唐詩數量: 34646\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import datetime\n",
    "from urllib.parse import quote\n",
    "from random import randint\n",
    "\n",
    "from flask import Flask, render_template, request, make_response\n",
    "from flask import jsonify\n",
    "\n",
    "import sys\n",
    "import time  \n",
    "import hashlib\n",
    "import threading\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.RNNPoet.poetry_porcess import *\n",
    "from src.RNNPoet.gen_poetry import *\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "batch_size = 64\n",
    "poetry_file = '../data/poems/poems.txt'\n",
    "words,poetry_vector,to_num,x_batches,y_batches = poetry_process()\n",
    "\n",
    "def start_requests(start_with, style_words=\"誰謂傷心畫不成，畫人心逐世人情\"):\n",
    "    ret_poem = \"\"\n",
    "    if len(start_with)>3:\n",
    "        ret_poem = generate_poet(start_with, words, to_num, style_words=\"大漠孤煙直，長河落日圓。\")\n",
    "        #print(new_poem)\n",
    "        items.append({\n",
    "          \"poems\": str(ret_poem),\n",
    "          \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "    else:\n",
    "        items.append({\n",
    "          \"poems\": \"start words need more characters.\",\n",
    "          \"updatetime\":datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        })\n",
    "    return str(ret_poem)\n",
    "\n",
    "def heartbeat():\n",
    "    print (time.strftime('%Y-%m-%d %H:%M:%S - heartbeat', time.localtime(time.time())))\n",
    "    timer = threading.Timer(60, heartbeat)\n",
    "    timer.start()\n",
    "timer = threading.Timer(60, heartbeat)\n",
    "timer.start()\n",
    "\n",
    "app = Flask(__name__,static_url_path=\"/static\") \n",
    "@app.route('/message', methods=['POST'])\n",
    "\n",
    "def reply():\n",
    "    start_with = request.form['msg']\n",
    "    res_msg = \"Res:\"\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    yesterday = datetime.datetime.strftime(datetime.datetime.now() - datetime.timedelta(1), '%Y-%m-%d')\n",
    "    ret_poem = start_requests(start_with, \"大漠孤煙直，長河落日圓。\")\n",
    "    rand_post_index = randint(0, len(items)-1)\n",
    "    print(items)\n",
    "    l = \"<font color=white>\"+ret_poem+\"</font>\"\n",
    "    res_msg = l\n",
    "    if res_msg == ' ':\n",
    "        res_msg = 'No Data input'\n",
    "\n",
    "    return jsonify( { 'text': res_msg } )\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index(): \n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "# 啟動APP\n",
    "if (__name__ == \"__main__\"):\n",
    "    pass\n",
    "    #items = []\n",
    "    #app.run(host = '127.0.0.1', port = 8898)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/poems_chatbot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "- 以三字經進行訓練 (./data/poems/three.txt)\n",
    "- 部署成聊天機器人"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
